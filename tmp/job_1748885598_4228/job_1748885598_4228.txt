You may not realize it, but when you're building AI agents, you're actually building microservices. Of course, not every AI agent has to be a microservice, but I believe it should, especially for production. Because to build any modern AI system, we will need multiple agents to exchange context while working independently to achieve the system goal. I'm Adi Polak with Confluent, and in this video, I'm going to briefly introduce what AI agents are and how you can build your modern AI architecture and take it to production, leveraging event-driven microservices. So what is an AI agent? AI agent, essentially, is a specific case of an AI application. Now, this agent will usually have tools, it will get a specific goal, and it will be able to act and plan in order to achieve objectives autonomously. And in order for us to get the best results out of each agent, each one of them has to be fine-tuned and focused to do one specific task. So overall, all these agents together build multi-agent systems that work collaboratively to achieve our goal. Now, if this sounds complicated, let's take a look at an example so we can better understand what it is that we're building. Let's assume we're working for Badflix. Badflix has movies that got the lowest rated score on the planet. We are responsible to build their agentic system. And here, we get different prompts in order to understand what's happening with our sales cycle. The first prompt that we're going to get is, predict sales for the next three months. Now, how do we take this prompt and transfer it into an actual answer? So this is my prompt. I'm going to ingest this text into a controller. A controller is a smart agent that can wake up and discover what different tools it has in its environment and what the resources are it can access. The controller will need to work with a planner. The planner is going to build a plan in order to answer the answer. Here, specifically, it's going to tell us that we need to start working against a database and turn some of these texts into a dedicated SQL statement that's going to look at the sales from the previous year. Now, we also need every agentic AI system to have some feedback loop. So here, specifically, we're going to tap into a judge. The judge is going to observe the input, the output, and what was the plan. And it's going to tell us if we did good or not. Here, specifically, the judge can come and say, hey, you know, it's actually in order to predict the sales for the next three months, we need to look at the marketing campaign. So we turn back a response saying, this answer is not good enough. And this is what makes this system a cyclical system with internal feedback loop. Now, our smart system also needs to handle churn. So now we're going to get a prompt saying, hey, build this alert for identifying churn customers, churn subscriptions a week in advance. So this is another prompt that I'm going to send to my controller. My controller is going to take it, it's going to process it, it's going to send it to the planner. The planner now is going to repeat and say, in order for me to identify churn from my entertainment system, I need to be able to access click streams. Because click streams tell me if this user specifically is actually using their platform. On top of that, I need access to chat history, I need access to support tickets, and so on. So I need to be able to process data streaming. This data streaming handler is going to give me access to process any real-time information. And not only that, now we have a new functionality that we need to address. And this is alerts. Based on this data streaming, I want to fire an alert that is going to trigger the system, that is going to tell me that now this is going to be in so-and-so churn within my subscribers. So Badflix can actually take an action based on that. This system overall is going to work. But what happens now when I need to change the planner, for example? I want to have a v2 of this planner. And if you think about it for a second, we actually, what we built here is a monolith. And monoliths are not bad, they can work. Each one of them can be a function, each one of them can be a class. But now that I want to upgrade my planner, that means that I'm going to need to go and deploy all of them together. So now I have dependencies on deployment timeline. There could be that I also have dependencies on hardware, because here, let's say this planner is smart enough to use an LLM, external resource that helps me calculate what I need. That means that the application itself can actually do well with a CPU, cheaper, easier to access. Now this SQL handler, it's small enough functionality here that I can actually use small language model, which means for this specific thing, I will need a GPU. So you can see how all these dependencies are already making it harder for me to scale the system. So I definitely need to move from monolith to a microservice. Now one of the ways to communicate in a microservice, is through request response. But we've done one before. We know this request response is going to get us stuck in slower system. Because actually I'm adding this dependency in between the planner and the controller, in between those entities. And so I want to move away from these dependencies. I want to completely decouple this system. This is where I want to start using event-driven architecture. What is an event-driven architecture? And what do I need in order to be successful with it? The first thing that I need, I will need an immutable event log. So here I have my infinite immutable event log and I will have an agent writing events and an agent consuming those events. And if it sounds familiar, it's because yeah, I mean the best system for managing events is absolutely Apache Kafka. So now I have these events. This is the first step of taking this architecture and turning it into something that is more applicable for production that I can actually scale. And now that I have Kafka, you know what? I can actually start tapping into Kafka Connect. And Kafka Connect will enable me to build this real-time knowledge system that will later on be available for all the agents to tap into. And now that we talked about knowledge systems, we can take some of these applications and perhaps tweak and tune it into a Flink job. So Flink enables us to do processing of all of this information. So here my Kafka topics are now being exposed to Flink as a table. So Flink, my data streaming processing engine. I can also use it for batch but this is on a different day. And one of the interesting functionality, it can work against an LLM. So Flink AI inference enables us to take some of the data, send it to an LLM, and get back a response. So this is very similar to the planner because now that we move this communication between the controller to the planner to actually be a Kafka topic here, Flink can take it and build the planner as a Flink app. And this is not all. Some of the challenges that we always have with these type of applications that are based on LLM is hallucinations. And hallucinations is happening because a lot of these LLMs were being trained on previous historical data. But now in order to give us a real answer, we want to ground that in factual data. And so there is a known pattern in the world named RAG. In order for me to build RAG, I need a vector database. A very interesting functionality that exists in Flink is actually my ability to turn this data that Flink takes and process into an embedding data. So Flink can actually tap into an LLM through the Flink inference model and ask the LLM to turn the data into a dedicated embedding data. Then later on, we can sync here into my Kafka topics and work through Kafka Connect in order to build the full agentic RAG pattern. So now my system is not only scalable, not only decoupled, but now this simple architecture that I've now built enables me to tap into this agentic RAG solution as well. All this access to data means that I need to be able to govern it. So I need a governed solution. Now I'm not going to give admin credentials to my agents. I actually need to start managing this access control. So govern is going to make sure I am managing this access control. On top of that, I actually need to be able to put the guardrails of input and output between all these communications. So this is where schema registry is going to help me put the guardrails in the right place. Beyond that, I want to have lineage and I want to have observability. Through lineage, I can tell what happened in my system, which event went where, how did they transform, who communicated with who, and that actually helps me tap into what I would want to have later in the future, observability for all of my system here. So this is how I took my monolith and I turned it not only to a microservice system, but into a smart microservice system, leveraging event-driven architecture. So what do we need to be successful with building AI agents? We actually need a data streaming platform. Call them agents, call them microservices. If they don't speak with events, they're just fancy applications with LLM brains waiting to become obsolete.