So I'm a big believer that the secret to building a successful directory is to become a master curator. That means that you want to focus on creating the highest quality listings on your site rather than just focusing on having as many listings as possible. If you can nail that, not just will you find it easier to attract people to your directory, you'll also have people coming back because you become a trusted authority in your niche. But a big challenge is how do you even find high quality data? And that is exactly what we're going to be talking about in today's video. Let's get into it. Hey friends, what's going on? Welcome back to the channel. So in today's video, we're going to be going over how to find high quality data for your directory website. If you are brand new here, last week I published a video where I broke down the exact process that I went through in order to create a directory website called the running directory using no code tools. If you have not seen that video yet, just jump back, check it out. It will add a whole lot more context to this video today and it will be a lot easier for you to get started. Now one of the questions that came up quite a bit in the last video is how do you find high quality data for your directory? Now if you go out and ask anyone who builds directories how they approach it, you'll typically get one of two answers. Answer number one is to find a high quality website or find an API and to then use a scraping tool to basically transfer all of that information into your own database. Now I'm personally not a huge fan of it simply because you end up in situations where you accidentally transfer over a whole bunch of data that's either out of date or inaccurate. And secondly, I have also found that the actual secret to managing data on your website long term is actually to have your customers manage it. And the way that you can get customers to manage your listing is to simply become an authority in your space and to just become a valuable resource for people who end up listing on your site. And the way that you get to that point is by setting a super high bar when it comes to the quality of listings on your site. So today what I'm going to be doing is I'm going to be breaking down four workflows that I use to one, find high quality data sources in order to find my first few listings, workflows that I use to enrich the data that I find. So that means finding image assets and to doing deeper research into the individual listings. And then how I also clean my data to make sure that one, I benefit from the SEO of the individual listings, but also that I can just improve the overall user experience for things like listings and search and filtering. Now, if you are thinking about building a directory, this video here is really the video that's going to set the foundation for things to come. If you can get the quality of your data right, everything else that follows after that will become so much easier. And so let's dive straight into the first point, which is to go over how I find data for my directory. Okay, so the first step in all of this is to find different data sources that you can use as inspiration for your directory. Now, I typically think the best place to get started is with Google. So let's say, for example, we are building a restaurant directory that specializes in gluten free food. So what I would typically do is I'd start off with a simple Google search. So I'd go gluten free restaurants in New York. And what you'll find is you'll get a whole bunch of different results. So for example, you'll have Google My Business listings, you'll have blogs, you'll have directory websites, you'll oftentimes get links to different Reddit posts. And all of these data sources here are really the key starting point to it all. What you'll want to do is you'll want to review different data sources to essentially see what it is that other people are listing. The main benefits from taking this approach is, number one, you'll actually see what some of your competitors will do. You'll be able to see what kind of data they have. But more importantly, it will serve as an inspiration for finding the first bit of data that you can then use on your site. Now, when it comes to finding good data sources, what I actually recommend is to find data sources that have structured sites. Now, what do I mean by this? Now, typically you'll find two different types of data sources when you're doing your research. One is what I considered unstructured data sources and the other one is considered a structured data source. Now, what do I mean by this? Now, an unstructured data source is usually something like a personal website. Now, the reason why it's unstructured is because the actual setup of the content on that particular site is unique to the website itself. However, on the other hand, you also have structured sites where the information and the content is laid out in the exact same way across different pages. For example, Instagram. If we go and have a look at Instagram, you will find that, in my case, a lot of rung clubs use Instagram to basically promote their rung clubs. And all of the information is structured in a very similar way. For example, every rung club on Instagram will have a profile image. Then they will also have a name. They'll also have a description and they'll also have an address and a link. Now, the main reason why you want to use structured data sources is because it will make the process of transferring data to your database a lot more efficient. And that is what we're going to be talking about next. Now, once you've found your data source, the goal is to get that data from your data source into your database in the most efficient way. Now, to just quickly recap on a tool that I'm using for my database from the last video, the tool that I'm using is Airtable. Now, the main reason why I use Airtable is just because it's a nice visual database tool. You don't really need to have any coding background and able to use it efficiently. But one of the things I didn't mention in that last video is that it also has a whole bunch of tools that makes adding data to your database super efficient. One of the tools that I use is called the Airtable Web Clipper. Now, the Airtable Web Clipper is a Chrome extension that you can simply install. So just Google Airtable Web Clipper Chrome extension, install it. And what that will allow you to do is when you're then browsing through different data sources, you can actually activate your Web Clipper and add a data directly to your database from inside of the browser. So, for example, here I can click on Add Record to Run Clubs. And what it will do is it will now allow me to add certain information. So, for example, I can add the name of the Run Club. So it'd be something like Achilles International. Then I can add a link to it. So if there's a link in there, I can just go ahead and copy the link address. If there's a logo there, I can also go ahead and just select the image from the page. So let's just use this one for now. And then if there's also a club description, I can just go ahead, add that in here, and add what any other information I want to my Web Clipper. So you can customize your Web Clipper in whatever way you want. You can add whatever fields you need. And so depending on what information you typically find from your data source, you just go ahead, add that in here, and then you can just click on Add Record. And what that will do is it will automatically add all of the information that you found and add it to your Web Clipper directly into your Airtable database. And so then it will go ahead and look like this. But one of the things that you can do with the Web Clipper that is a complete game changer and will speed up this workflow significantly is to pre-populate your Web Clipper with data on certain pages. Now, the way that this works is that you can simply add your Web Clipper extension inside of Airtable and you can predetermine what you want certain fields to be populated with. And so you can do something, for example, like if you want to populate the Learn More link, you can actually just take the actual page URL that you're on. If you, on the other hand, want to populate something like a name, the one thing that you can do that is super powerful is actually use CSS selectors, meaning that you're actually targeting the class name of a particular element on a website and you're then extracting the content from that class name. And this works particularly well on structured sites. So if, for example, I go to Instagram, because all of the information is on the same page and it's using the same class names because it's essentially using a CMS template page, we can actually target different elements on this page using our CSS selectors. Now, this here is slightly technical, but basically, if we go and have a look at the source code and we want to see what the image is, you'll see that the actual image is within a certain class. And so what we can do is we can simply say to our Web Clipper, hey, target the content in the class that has this class name. And what it will do is when you then open up your Web Clipper, it will automatically target the classes that you specified and pre-populate your content. So, for example, if I add it here, it will automatically find the name of the Run Club, which is this, the link, which is just the URL that I'm on. It will also bring in the logo. So if I click onto it, you can see here it just automatically added the logo. It will also add the run description if you want. I haven't set that up here. And then it will also add just whatever information you need. This here will make the process of adding data to your database so much more efficient because all of the information is pretty much pre-populated. And so you can go ahead, see that. And that is really the first step that you can take in order to efficiently populate your database with information. Now, as you're going through the process of adding all of these different data from your data source into your air table base, you'll find that a lot of the information that you're actually pulling from these sites are incomplete. So, for example, you might be missing things like an image that you might want to use on your listing site. It might be missing links to certain types of social accounts. And so what you want to do after you start populating your database is move on to the next step in managing your data for your directory, which is to enrich your data. Now, enriching your data basically means to add additional information or assets to your listings that will make your overall listing quality way better. Now, depending on what it is that you're doing, you're going to have to probably take a slightly different approach from me. But I've got two workflows that I use that speed up the process for enriching my data significantly. Now, the first one that I'm going to show you is what I have used in order to get images and logos for different races that I'm listing on my running directory. And that is with a tool called Apify. So, Apify is a tool that you can use to scrape data from various sources. So if we're going to have a look here, you can see that you've got a way to scrape and download Instagram posts. You can get information from, you know, Facebook posts and really they've got tons and tons of different actors. And basically what it does is it runs a script that basically retrieves information about certain, you know, data that is hosted online. Now, my use case was going to be to find the Facebook pages for the different races that I was trying to promote and to then pull information from that particular Facebook page. Now, the reason why I chose Facebook is because when I was doing my research, I noticed that pretty much all of the races used Facebook to promote the event. One, because it's obviously free to use. But two, if you wanted to use any form of paid advertising on Facebook or Instagram, you are required to have a Facebook page. And so when I was doing my research, I found that most races uploaded really high quality race images as their banner image. Plus, I had a logo that I could then also use in my race card on the running directory. And so what I can then do with a tool like Apify is to essentially run a workflow that pulls the information from that particular Facebook page and then allows me to store it inside of my database. Now, let me show you a quick little example for how this works. I'm going to use in this example, my own website, my own Facebook page, the Unicorn Factory NZ. All I need to do is just define the URL of the Facebook page, which is why it's important to find data sources where you can potentially find social links. And then you just go ahead and run it. Now, this process usually takes anywhere between 10 to 15 seconds. And as soon as the run is done, you'll see all of the information that was found. So, for example, it will tell you a thing like the business service area. It will retrieve the cover photo. It will also retrieve, you know, the profile image. And it's basically data that you can now use to store inside of your database in order to enrich your listing. Now, here's where Apify gets particularly powerful. Now, not just do you run it through the actual Apify interface. You can actually connect your Apify account to a make.com workflow and you can then basically automate this entire process. So, if you found a way to find a social link or in my case a Facebook link for a particular running event, then what I can do is I can actually initiate my Apify actor to basically conduct the Facebook search. And so you need to set this up in two workflows. Number one is to go out there and actually specify the URL of the page that you want to run. And so what I've done is I've basically connected it to a web hoc that then retrieves the air table record that I want to do research on or I want to enrich. And it will then pass through the Facebook URL and then initiate the search. The next step in it is to then set up a separate workflow where it monitors for different runs. So, basically monitors whether or not my actor found a result. And then once it does, it goes through the process of retrieving all of the data that was found. So, basically what I showed you before in Apify. It then finds the associated air table record by the actual Facebook URL and it then goes ahead and it updates the listing that I wanted to enrich with information like the cover photo URL, with the website URL and a whole bunch of other data. That is a super simple and effective way to enrich your data with information that you can then later on use on your site. Now, the second tool that I use to enrich data is going to be an absolute crazy one. So, if you have made it to this point of the video, congratulations. I actually think that this one here is really what will allow you to take the quality of your listings from, you know, 60% to like 90%. So, this here is going to be an absolute sick one. So, one of the things that you're going to run into is that as you're pulling data in from different data sources, again, there's going to be so much information missing that might be important to create, you know, for example, listing pages. Now, for example, with the different races that I found, I would find things like the name of the race, you know, the title, where it was, maybe the event date, and that was pretty much it. But if I really want to enrich the data, what I want to do is do even more research. And so, in this particular case, what I was looking for is to find out, hey, what are the actual individual events that are going on during this particular race weekend? And a tool that you can use to do deeper research is a tool called Perplexity AI. Now, Perplexity AI works in a very similar way to ChatGPT, with the main difference being that instead of it just using, you know, the data that it was trained with to provide the answer, it actually goes and looks for up-to-date and accurate sources. So, what I could do is I could basically jump in here and say, can you tell me what running events are on during, and then I'll just go ahead, paste the name of the event and where it is. And then I can specify things like, tell me the distance, time of start, and entry fee, if you can find it. And then what I'll do is I'll just go ahead, I'll go and make a search, and what it will do is it will go and research all of these different data sources. So, it will look up Instagram, it will look up Facebook, it will look up a whole bunch of different sources. It will look up the actual race website as well. And what it will do is it will return all of the information. So, for example, it will tell me, hey, it's scheduled for this date, it's got these events. So, we've got, you know, like the road race, and then we've got the distance, and we've got the entry fee, and we've got the start times, and all that kind of stuff. And I can now use all of this information to now take my data even further. And so, what you can do is you can now plug the particular workflow that I just showed you into a make.com workflow. Okay, so here's how the automated version of this would look. The first thing is I'll trigger my workflow from inside of your table. I'll then retrieve the information about the race that I want to do deeper research on. And then what I'll do is I'll connect my Perplexity AI prompt. And so, basically, what I'm doing is I'm asking the exact same question that I just showed you in the interface. I'm saying, hey, can you get me all of the information about the following running events in this format? So, the race name, distance, start date, start time, registration link, and entry fee. Then what I do is it will go and do its research. It will find a response, and then I use an OpenAI integration to take that response and to format it in a way that makes it easier for me to store inside of my database. So, for example, I'll say, hey, I need you to transform race details into a JSON array. Each individual race should include the following keys. Race name, race distance, race date, race time, event link, entry fee. And what it will do then is it will go ahead and it will take the message or the response from Perplexity AI and transform it into that format. Now, another thing that I do, which I think is super important, is to actually save these citations. So, all of the links that it provides you, it will actually give you in the API call so that you can then later on reference it to see where that data is coming from. Once that is done, what you can do is you can then take the information and store it inside of your ear table database. Now, in my case, what I did was I actually created a separate table for the individual race events. And once I have those individual race events, it basically becomes a linked record that I can then later on use to then basically create more detail-rich profile pages for individual races. And so this is basically what this ends up looking like. It's a link to a whole different table and it's got all of the information that was retrieved by the API call. And so essentially what I'm doing is I'm doing deeper research here and I'm using different sources online to basically retrieve information that I can then use to store even more data inside of your table. Now, as you can imagine, the use cases for this are endless. If you want to do deeper research into, for example, the business that you're listing on your site, you can just run a workflow like this. You can specify what information you're looking for and then you can just store it inside of your database. This is how you go from having a content-poor site or content-poor listing to having a super content-rich listing. It goes without saying that you need to keep an eye on the data that's being imported because from time to time it's using data sources or data points that are inaccurate. And so you just want to make sure that you have a way to keep an eye on what's going on. But this here is a complete game changer when it comes to importing data. OK, now, before I show you the last step in managing all your data, I wanted to quickly let you know that I am actually creating the written version of this guide on my website. So with this last video, I basically posted it in here. And basically, if you're someone that just needs this in written format, it's all in here. Not just do I add information from the actual video in here. I'll also be adding things like scripts when it comes up. I'll also be adding links to different resources. So one thing that I would ask you to do if you are interested in learning more about this is to just head on over to my website. Subscribe to my inbox so that I can notify you whenever new things are coming out like new guides that I'm creating or, you know, whenever I'm updating this. And also, if you have questions about particular videos, send me an email. OK. And then what I'll do is I'll add it to a frequently asked questions section that I'll add to each video where I go into more details answering questions that you might have. Another place to ask questions is also inside of the comments section on Facebook. I'd appreciate questions there a lot because it helps with engagement. But with that being said, let's move on to the last point in populating your database with high quality data, which is to make sure that your data is clean. OK. So the final step that we need to take care of when it comes to making sure that the content in our directory is of the absolute highest standard is to clean our data. Now, what does this mean? So the first step that we went through was to find and populate our database. Then we use Perplexity and APIify to enrich our data. But one of the things that you're going to notice is that as you're importing data from different sources, it's just going to import in different formats. Now, this was an issue that I ran into with, you know, importing race information specifically. What will happen is you're just going to get the information in a whole bunch of different formats. So, for example, your date string might be formatted incorrectly. Sometimes certain information won't be imported at all. One thing that I found myself in, you know, a lot is, you know, just how the race event distances were specified. So sometimes it would say half marathon. Other times it would say the exact distance. Other times it would like specify the distance in miles. Other times it would specify as 5K. So ultimately all of the ways that the information was provided from these different sources was super inconsistent. And as you are importing inconsistent data into your actual directory site, it can lead to issues with the overall user experience. So for starters, it just looks unprofessional, you know. It just looks inconsistent and it just kind of looks like, you know, just slapped some data in there. But more importantly, the problem that you'll run into is when it comes to having features like search and filtering. So at the moment, if I, you know, select my search and filters, everything works fine because I've actually formatted all of the information in a way that's consistent. So a really specific example is if I were to like import this race without formatting the information, it actually does have a half marathon in here but it's specified as 21.097 kilometers, which means that if I were to filter it by half marathon, it would simply not show up because the actual event itself is not specified as such in my listing. Now you're going to run into this issue a lot in a lot of different ways but this problem is very easily fixed by using scripts. Now this is going to be another example of why Airtable is super valuable. But what we can use in Airtable is a feature called Airtable Automations that allows us to set up workflows that we can use for, in this case, data cleaning. So the way it works is we head over to our automation tab and in here you'll see that we have access to basically an automation tool like make.com or Zapier. Now what I'm doing here is I am setting up a workflow that has a trigger. So in my case, the trigger is when I set the action field inside of my run clubs to enrich data. And then what I do is I use the scripting block. Now the scripting block allows you to simply make a JavaScript script that you can then use to do various different things. Now the reason why this is so powerful is because it actually allows you to make API calls to third-party tools. Now an API call that I am making in this particular example is to OpenAI. And OpenAI is the tool that I'm using to format all of my information. And so you can see in here I've got a prompt that basically describes what it is that I want OpenAI to do. So what I'm saying is, hey, you're an assistant that analyzes text and returns structured data in JSON format. Then I say, please analyze the following run club description and extract the following information in JSON format. And then I basically say, I want you to tell me the city, and this is the format I want it in. Then I want you to tell me the meeting days and times. Then this is the format I want you to provide it in. Then I want you to provide me the meeting location. This is the format I want it in. And then I want you to also find tags. And this is another use case for data cleaning. If you have something like tags, then you can also ask it to basically generate tags based on something like a description. So if, for example, there's a beer run club, you can apply that tag, and it will then automatically apply when you're filtering. And so what you can do here is you can really take the input that you're getting from these different data sources and then using a data cleaning workflow like this to basically format it in a way that will work across all of your different tools. And that is simply how it works. Now, just to help you understand how I actually generate this script, I do have a little bit of an understanding of how these scripts work. But if you describe to ChatGPT what you want your script to do, you can very easily recreate these. And you will likely run into errors, but just ask it to basically provide you console logs along the way, which is basically it prints out the results from each step, and then you'll always see where the issue is. And so ultimately what I have done here is I have specified the record that I want to update. I've then pulled in information about that particular record, for example, the description. And then I made my API call to go ahead and to basically format it based on the description that was pulled, and it will just show you along the way what it finds. So it found this record, then it found this description, then it made the call to OpenAI, and then it returned my results, and then it passed my results and stored it back into the respective fields inside of YearTable. And just like that, you are able to take your data and clean it. Okay, and so those are the four workflows that I'm currently using to find and populate my database, to enrich my data with more information and assets, and to then also clean my data to make sure that the overall usability on my site is better. Now, as you can see, this process is definitely a bit more manual than just getting a scraping tool and pulling mass amounts of data from different sources, but it allows you to make sure that the overall quality of your data is significantly higher, and that is ultimately what I think is the key to success when it comes to building great directory sites or businesses in general. Now, this is it for this video. If there are any questions, please do let me know. Also, go and check out my written guide for this particular video where I'll be basically linking to different resources and all those types of things. And also, please let me know in the comments down below what the next thing is that you want to learn about building out No-Code directory sites. I am super keen to make more videos around this, and if you have any specific questions or challenges that you have along the way, I am more than happy to help. With that being said, thank you so much for sticking around for the entire video, and I'll see you back here for the next one.